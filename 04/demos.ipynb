{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Pneumonia with Chest X Ray Images Using a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Used** - https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\n",
    "\n",
    "**Citation:**\n",
    "Kermany, Daniel; Zhang, Kang; Goldbaum, Michael (2018), “Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification”, Mendeley Data, V2, doi: 10.17632/rscbjbr9sj.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting main folder\n",
    "\n",
    "main_fol = \"data/chest_xray/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test and val folders\n",
    "\n",
    "train_folder = os.path.join(main_fol,\"train\")\n",
    "test_folder = os.path.join(main_fol,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pneumonia and normal image paths\n",
    "\n",
    "pneumonia_train_images = glob.glob(train_folder+\"/PNEUMONIA/*.jpeg\")\n",
    "normal_train_images = glob.glob(train_folder+\"/NORMAL/*.jpeg\")\n",
    "\n",
    "pneumonia_test_images = glob.glob(test_folder+\"/PNEUMONIA/*.jpeg\")\n",
    "normal_test_images = glob.glob(test_folder+\"/NORMAL/*.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating traing and test dataframes\n",
    "\n",
    "train_list = [x for x in normal_train_images]\n",
    "train_list.extend([x for x in pneumonia_train_images])\n",
    "\n",
    "df_train = pd.DataFrame(np.concatenate([['Normal']*len(normal_train_images) , ['Pneumonia']*len(pneumonia_train_images)]), columns = ['class'])\n",
    "df_train['image'] = [x for x in train_list]\n",
    "\n",
    "test_list = [x for x in normal_test_images]\n",
    "test_list.extend([x for x in pneumonia_test_images])\n",
    "\n",
    "df_test = pd.DataFrame(np.concatenate([['Normal']*len(normal_test_images) , ['Pneumonia']*len(pneumonia_test_images)]), columns = ['class'])\n",
    "df_test['image'] = [x for x in test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.title('Number of cases', fontsize=12)\n",
    "sns.countplot(df_train['class'], data=df_train)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting some sample images\n",
    "\n",
    "# normal images\n",
    "fig, axes = plt.subplots(nrows=1, ncols=6, figsize=(15,10))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = cv2.imread(normal_train_images[i])\n",
    "    img = cv2.resize(img, (512,512))\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(\"Normal\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "fig.tight_layout()    \n",
    "plt.show()\n",
    "\n",
    "#pneumonia images\n",
    "fig, axes = plt.subplots(nrows=1, ncols=6, figsize=(15,10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = cv2.imread(pneumonia_train_images[i])\n",
    "    img = cv2.resize(img, (512,512))\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(\"Pneumonia\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "fig.tight_layout()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting training and testing data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df_train, test_size = 0.20, random_state = 13, stratify = df_train['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset using ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "                rescale = 1/255)\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "              rescale = 1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col=\"image\",\n",
    "    y_col=\"class\",\n",
    "    target_size=(150,150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    seed=7\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    x_col=\"image\",\n",
    "    y_col=\"class\",\n",
    "    target_size=(150,150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    seed=7\n",
    ")\n",
    "\n",
    "test_generator = val_datagen.flow_from_dataframe(\n",
    "    df_test,\n",
    "    x_col=\"image\",\n",
    "    y_col=\"class\",\n",
    "    target_size=(150,150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    seed=7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary packages\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the CNN\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# convolution\n",
    "model.add(Conv2D(filters=16, kernel_size=(3,3), activation=\"relu\", input_shape=(150,150,3)))\n",
    "\n",
    "# pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# 2nd conv\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation=\"relu\"))\n",
    "\n",
    "# 2nd pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# 3rd Conv\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "# 3rd Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# fully connected layers\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of parameters are calculated like this:\n",
    "\n",
    "**1. For Conv. Layers:**\n",
    "   \n",
    "   no. of parameters = output_channels * (input_channels * window_size + 1)\n",
    "   \n",
    "   \n",
    "**2. For Dense Layers:**\n",
    "    \n",
    "   no. of parameters = output_size * (input_size + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model and validation with testing data\n",
    "\n",
    "cnn_model = model.fit(\n",
    "            train_generator,\n",
    "            epochs=10,\n",
    "            validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(cnn_model.history['loss'], label='Training Loss')\n",
    "plt.plot(cnn_model.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Evolution')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(cnn_model.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(cnn_model.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Evolution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")\n",
    "\n",
    "evaluation = model.evaluate(train_generator)\n",
    "print(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_generator.classes\n",
    "y_pred = (model.predict(test_generator) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\")\n",
    "\n",
    "plt.xlabel(\"Predicted Label\", fontsize= 12)\n",
    "plt.ylabel(\"True Label\", fontsize= 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, Recall and F1-Score of the model\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = (2*precision*recall/(precision+recall))\n",
    "\n",
    "print(\"Recall of the model is {:.2f}\".format(recall))\n",
    "print(\"Precision of the model is {:.2f}\".format(precision))\n",
    "print('F1-score: {}'.format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen_2 = ImageDataGenerator(\n",
    "                rescale = 1./255,\n",
    "                shear_range = 0.2,\n",
    "                zoom_range=0.2,\n",
    "                horizontal_flip=True,\n",
    "                rotation_range=10,\n",
    "                fill_mode=\"nearest\")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "              rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_2 = train_datagen_2.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col=\"image\",\n",
    "    y_col=\"class\",\n",
    "    target_size=(150,150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    seed=7\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    x_col=\"image\",\n",
    "    y_col=\"class\",\n",
    "    target_size=(150,150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    seed=7\n",
    ")\n",
    "\n",
    "test_generator = val_datagen.flow_from_dataframe(\n",
    "    df_test,\n",
    "    x_col=\"image\",\n",
    "    y_col=\"class\",\n",
    "    target_size=(150,150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    seed=7,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "lr_reduce = ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.1, min_delta=0.0001, patience=1, verbose=1)\n",
    "\n",
    "filepath=\"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=\"val_accuracy\", verbose=1, save_best_only=True, mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation=\"relu\", padding=\"same\",\n",
    "                 input_shape=(150,150,3)))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation=\"relu\", padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\", padding=\"same\"))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\", padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(rate=0.6))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "\n",
    "model_htuning = model.fit(\n",
    "            train_generator_2,\n",
    "            epochs=10,\n",
    "            validation_data=val_generator,\n",
    "            callbacks=[lr_reduce,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(model_htuning.history['loss'], label='Training Loss')\n",
    "plt.plot(model_htuning.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Evolution')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(model_htuning.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(model_htuning.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Evolution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")\n",
    "\n",
    "evaluation = model.evaluate(train_generator_2)\n",
    "print(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_generator.classes\n",
    "y_pred = (model.predict(test_generator) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\")\n",
    "\n",
    "plt.xlabel(\"Predicted Label\", fontsize= 12)\n",
    "plt.ylabel(\"True Label\", fontsize= 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, Recall and F1-Score of the model\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = (2*precision*recall/(precision+recall))\n",
    "\n",
    "print(\"Recall of the model is {:.2f}\".format(recall))\n",
    "print(\"Precision of the model is {:.2f}\".format(precision))\n",
    "print('F1-score: {}'.format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet152V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_base_model = ResNet152V2(input_shape=(150, 150, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "resnet_base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer learning network\n",
    "\n",
    "model_tl = Sequential()\n",
    "model_tl.add(resnet_base_model)\n",
    "model_tl.add(Flatten())\n",
    "\n",
    "model_tl.add(Dense(1024,activation=\"relu\"))\n",
    "model_tl.add(BatchNormalization())\n",
    "model_tl.add(Dropout(rate=0.5))\n",
    "\n",
    "model_tl.add(Dense(128,activation=\"relu\"))\n",
    "model_tl.add(BatchNormalization())\n",
    "model_tl.add(Dropout(rate=0.4))\n",
    "\n",
    "model_tl.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze initial layers of the network\n",
    "resnet_base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tl.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tl_final = model_tl.fit(\n",
    "            train_generator_2,\n",
    "            epochs=10,\n",
    "            validation_data=val_generator,\n",
    "            callbacks=[lr_reduce,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(model_tl_final.history['loss'], label='Training Loss')\n",
    "plt.plot(model_tl_final.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Evolution')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(model_tl_final.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(model_tl_final.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Evolution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model_tl.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")\n",
    "\n",
    "evaluation = model_tl.evaluate(train_generator_2)\n",
    "print(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_generator.classes\n",
    "y_pred = (model_tl.predict(test_generator) > 0.5).astype(\"int32\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\")\n",
    "\n",
    "plt.xlabel(\"Predicted Label\", fontsize= 12)\n",
    "plt.ylabel(\"True Label\", fontsize= 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, Recall and F1-Score of the model\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = (2*precision*recall/(precision+recall))\n",
    "\n",
    "print(\"Recall of the model is {:.2f}\".format(recall))\n",
    "print(\"Precision of the model is {:.2f}\".format(precision))\n",
    "print('F1-score: {}'.format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
